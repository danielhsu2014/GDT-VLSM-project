<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="title" content="Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems">
  <meta name="description" content="Vision-Language Simulation Models that generate executable FlexScript from layout sketches and prompts, trained on 120K prompt–sketch–code triplets with new structural metrics.">
  <meta name="keywords" content="digital twins, vision-language models, simulation, industrial systems, FlexSim, code generation, multimodal learning">
  <meta name="author" content="YuChe Hsu, AnJui Wang, TsaiChing Ni, YuanFu Yang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <meta property="og:type" content="article">
  <meta property="og:site_name" content="National Yang Ming Chiao Tung University">
  <meta property="og:title" content="Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems">
  <meta property="og:description" content="Vision-Language Simulation Models (VLSM) that synthesize executable FlexScript from layout sketches and natural-language prompts for industrial simulation.">
  <meta property="og:url" content="https://danielhsu2014.github.io/GDT-VLSM-project/">
  <meta property="og:image" content="static/images/first_pic.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Generative Digital Twins - overview">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems">
  <meta name="twitter:description" content="VLSM models generate executable FlexScript from layout sketches and prompts, evaluated with SVR, PMR and ESR.">
  <meta name="twitter:image" content="static/images/first_pic.png">

  <meta name="citation_title" content="Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems">
  <meta name="citation_author" content="YuChe Hsu, AnJui Wang, TsaiChing Ni, YuanFu Yang">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="">

  <meta name="theme-color" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems</title>

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
</head>
<body>

  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <h1 class="title is-1 publication-title">
              Generative Digital Twins:<br>
              Vision-Language Simulation Models for Executable Industrial Systems
            </h1>

            <div class="is-size-5 publication-authors">
              <span class="author-block">YuChe Hsu,</span>
              <span class="author-block">AnJui Wang,</span>
              <span class="author-block">TsaiChing Ni,</span>
              <span class="author-block">YuanFu Yang</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Institute of Artificial Intelligence Innovation, National Yang Ming Chiao Tung University
              </span>
            </div>

            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2512.20387"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.20387"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/danielhsu2014/GDT-VLSM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body has-text-centered">
        <figure>
          <img src="static/images/first_pic.png"
               alt="Overview of the Generative Digital Twins framework"
               style="max-width: 100%; border-radius: 12px;">
          <figcaption class="subtitle has-text-centered" style="margin-top: 1rem;">
            Generative Digital Twins pipeline: from layout sketch and natural-language prompt to executable FlexScript and industrial simulation.
          </figcaption>
        </figure>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We introduce Vision-Language Simulation Models (VLSM) that synthesize executable FlexScript from layout
              sketches and natural-language prompts for industrial simulation systems. Our framework is trained on
              <b>GDT-120K</b>, the first large-scale dataset of more than 120,000 prompt–sketch–code triplets that couple
              textual descriptions, spatial structures, and simulation logic. To evaluate generated scripts beyond surface
              text similarity, we propose three task-specific metrics: Structural Validity Rate (SVR), Parameter Match Rate
              (PMR), and Execution Success Rate (ESR), which jointly assess structural integrity, parameter fidelity, and
              simulator executability. Systematic ablations over visual encoders, connector modules, and code-pretrained
              backbones show that the proposed VLSM family achieves near-perfect structural accuracy and robust execution,
              establishing a foundation for generative digital twins that unify visual reasoning and language-based program
              synthesis.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Dataset Construction and Timing Distributions</h2>
      <div class="content has-text-justified">
        <p>
          The GDT-120K dataset is built from real factory layouts and control logic, with each sample pairing a hand-drawn
          layout sketch, a natural-language description, and executable FlexScript. Statistical fitting over factory logs and
          engineering heuristics is used to instantiate realistic interarrival and service-time distributions, ensuring that
          the generated code can be executed directly in FlexSim without manual repair.
        </p>
      </div>

      <figure style="margin-top: 1.5rem;">
        <img src="static/images/dataset_pipeline.png"
             alt="GDT-120K dataset construction workflow"
             style="max-width: 100%; border-radius: 12px;">
        <figcaption class="subtitle has-text-centered" style="margin-top: 0.75rem;">
          GDT-120K dataset construction: from curated production layouts and scheduling rules to prompt–sketch–code
          triplets that capture both spatial structure and simulation logic.
        </figcaption>
      </figure>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Vision-Language Simulation Models</h2>

      <div class="content has-text-justified">
        <p>
          The proposed VLSM family couples an OpenCLIP visual encoder with code-pretrained language models through a
          lightweight connector module. Layout sketches are encoded as a set of visual tokens that condition FlexScript
          generation together with the textual prompt, allowing the model to reason jointly about topology, routing, and
          timing parameters. Two backbones are explored: TinyLLaMA-1.1B for compact deployment and StarCoder2-7B for
          stronger code priors.
        </p>
      </div>

      <figure style="margin-top: 1.5rem;">
        <img src="static/images/VLSM_Pipeline.png"
             alt="Vision-Language Simulation Model architecture"
             style="max-width: 100%; border-radius: 12px;">
        <figcaption class="subtitle has-text-centered" style="margin-top: 0.75rem;">
          VLSM architecture: an OpenCLIP encoder extracts visual tokens from the layout sketch, which are projected by a
          connector and concatenated with text tokens before being decoded by a code-pretrained LLM into executable
          FlexScript.
        </figcaption>
      </figure>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Prototype Interface</h2>

      <div class="content has-text-justified">
        <p>
          To make the model usable by non-expert users, we implement a simple prototype interface where engineers can
          upload a layout sketch, type a short natural-language description of the production logic, and obtain
          executable FlexScript that can be loaded into FlexSim. This interface exposes VLSM as a “layout-and-prompt in,
          digital twin out” tool for rapid what-if analysis.
        </p>
      </div>

      <figure style="margin-top: 1.5rem;">
        <img src="static/images/UI.png"
             alt="User interface for Generative Digital Twins"
             style="max-width: 100%; border-radius: 12px;">
        <figcaption class="subtitle has-text-centered" style="margin-top: 0.75rem;">
          Prototype interface for generative digital twins. Users provide a sketch and a short description, and VLSM
          returns FlexScript that instantiates the corresponding factory in FlexSim.
        </figcaption>
      </figure>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Qualitative Results</h2>

      <div class="content has-text-justified">
        <p>
          Qualitative comparisons highlight that VLSM captures both high-level flow logic and fine-grained parameterization.
          The model recovers workstation topology, buffering strategy, and AGV routing from sketches that contain only
          coarse geometric cues. Generated FlexScript consistently satisfies the proposed SVR, PMR, and ESR metrics, and
          produces factory simulations that are visually and functionally aligned with the ground-truth models.
        </p>
      </div>

      <figure style="margin-top: 1.5rem;">
        <img src="static/images/Fig5_3.png"
             alt="Main qualitative examples of layout-to-FlexScript generation"
             style="max-width: 100%; border-radius: 12px;">
        <figcaption class="subtitle has-text-centered" style="margin-top: 0.75rem;">
          Qualitative examples on held-out layouts. For each case, a sketch and textual description are converted into
          FlexScript that reproduces correct routing, resource assignment, and timing parameters when executed in FlexSim.
        </figcaption>
      </figure>

      <figure style="margin-top: 2rem;">
        <img src="static/images/5.2Quant.png"
             alt="Comparison across different LLM backbones on the same factory layout."
             style="max-width: 100%; border-radius: 12px;">
        <figcaption class="subtitle has-text-centered" style="margin-top: 0.75rem;">
          Comparison of eight language backbones on a single production line. TinyLLaMA-1.1B and StarCoder2-7B closely
          match the ground-truth digital twin, while generic LLaMA variants omit machines or misplace buffers, confirming
          that code-specialized models are critical for executable digital twins.
        </figcaption>
      </figure>

      <figure style="margin-top: 2rem;">
        <img src="static/images/suppl_2.png"
             alt="Additional qualitative example with AGV-based layout"
             style="max-width: 100%; border-radius: 12px;">
        <figcaption class="subtitle has-text-centered" style="margin-top: 0.75rem;">
          Additional example with AGV-based material handling and shared buffers. VLSM infers the number of vehicles,
          docking logic, and buffer capacities from multimodal inputs, yielding structurally valid and executable code.
        </figcaption>
      </figure>

      <figure style="margin-top: 2rem;">
        <img src="static/images/suppl_3.png"
             alt="Additional qualitative example with mixed manual and automated stations"
             style="max-width: 100%; border-radius: 12px;">
        <figcaption class="subtitle has-text-centered" style="margin-top: 0.75rem;">
          Example with mixed manual and automated stations. The generated digital twin preserves re-entrant flows and
          complex precedence constraints, demonstrating that the model scales to dense production lines with heterogeneous
          workstation types.
        </figcaption>
      </figure>
    </div>
  </section>

  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a>, which was adapted from the
              <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website; we only ask that you link back to the template in the footer.
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>
</html>
